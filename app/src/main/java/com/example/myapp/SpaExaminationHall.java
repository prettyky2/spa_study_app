package com.example.myapp;

import android.content.Intent;
import android.content.SharedPreferences;
import android.content.pm.PackageManager;
import android.media.AudioFormat;
import android.media.AudioRecord;
import android.media.Image;
import android.media.MediaRecorder;
import android.os.Bundle;
import android.os.CountDownTimer;

import android.util.Log;

import java.io.InputStream;
import java.util.List;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;
import java.io.FileInputStream;

import android.Manifest;

import android.util.TypedValue;
import android.view.View;
import android.widget.Button;
import android.widget.ImageView;
import android.widget.TextView;
import android.widget.Button;
import android.widget.ImageView;
import android.widget.TextView;
import android.media.MediaPlayer;

import androidx.appcompat.app.AppCompatActivity;
import androidx.activity.EdgeToEdge;
import androidx.constraintlayout.widget.ConstraintLayout;
import androidx.core.app.ActivityCompat;
import androidx.core.content.ContextCompat;
import androidx.core.graphics.Insets;
import androidx.core.view.ViewCompat;
import androidx.core.view.WindowInsetsCompat;
import androidx.core.graphics.Insets;
import androidx.core.view.ViewCompat;
import androidx.core.view.WindowInsetsCompat;

import com.google.auth.oauth2.GoogleCredentials;

import com.google.protobuf.ByteString;
import com.google.android.material.progressindicator.LinearProgressIndicator;
import com.google.api.gax.rpc.StreamController;
import com.google.api.gax.rpc.ResponseObserver;
import com.google.api.gax.rpc.BidiStreamingCallable;
import com.google.api.gax.rpc.ApiStreamObserver;
import com.google.cloud.speech.v1.SpeechClient;
import com.google.cloud.speech.v1.StreamingRecognizeRequest;
import com.google.cloud.speech.v1.StreamingRecognizeResponse;
import com.google.cloud.speech.v1.StreamingRecognitionConfig;
import com.google.cloud.speech.v1.StreamingRecognitionResult;
import com.google.cloud.speech.v1.RecognitionConfig;

import io.grpc.stub.StreamObserver;

import com.google.api.gax.rpc.BidiStreamingCallable;
import com.google.api.gax.rpc.ApiStreamObserver;
import io.grpc.stub.StreamObserver;
import com.google.cloud.speech.v1.*;
import android.media.AudioRecord;
import android.media.AudioFormat;
import android.media.AudioTrack;
import android.media.AudioManager;
import android.util.Log;
import android.widget.Toast;

import java.io.ByteArrayOutputStream;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;


public class SpaExaminationHall extends AppApplication implements View.OnClickListener {

    private static final String TAG = "spaExaminationHall";
    private GoogleCredentials googleCredentials; // ğŸ”¹ ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸
    private static final int REQUEST_RECORD_AUDIO_PERMISSION = 200;
    private TextView questionNumber = null;
    private LinearProgressIndicator progressBar;
    private Button playPause = null;
    private Button playNext = null;
    private ImageView viewImage = null;
    private MediaPlayer mediaPlayer;
    private CountDownTimer timer;
    private TextView userAnswer = null;
    private int testNum = 0;
    private int currentIndex = 1;
    private int remainingTime = 30;
    private boolean isPaused = false;
    private boolean isMediaPlaying = false;
    private boolean isMediaPlayNow = false;
    private MediaRecorder recorder;
    private SpeechClient speechClient;
    private boolean isListening = false;
    private ScheduledExecutorService executorService;
    private ApiStreamObserver<StreamingRecognizeRequest> requestObserver;
    private String answer_1 = null;
    private String answer_2 = null;
    private String answer_3 = null;
    private String answer_4 = null;
    private String lastRecognizedText = ""; // ğŸ”¹ ë§ˆì§€ë§‰ìœ¼ë¡œ ì¸ì‹ëœ ë¬¸ì¥ì„ ì €ì¥
    private ConstraintLayout dailySpaExaminationHallUserAnswer;
    private ByteArrayOutputStream byteArrayOutputStream;
    private AudioRecord audioRecord;
    private boolean isRecording = false;
    private String recordedAudioFilePath_1;
    private String recordedAudioFilePath_2;
    private String recordedAudioFilePath_3;
    private String recordedAudioFilePath_4;
    SharedPreferences prefs;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        EdgeToEdge.enable(this);
        setContentView(R.layout.activity_spa_examination_hall);
        ViewCompat.setOnApplyWindowInsetsListener(findViewById(R.id.spa_examination_hall_activity), (v, insets) -> {
            Insets systemBars = insets.getInsets(WindowInsetsCompat.Type.systemBars());
            v.setPadding(systemBars.left, systemBars.top, systemBars.right, systemBars.bottom);
            return insets;
        });

        initializeClass();
        startPlayback();

    } //onCreate()

    @Override
    public void onClick(View v) {
        if(v.getId() == R.id.btn_play_pause) {
            togglePause();
        } else if (v.getId() == R.id.btn_play_next) {
            skipToNext();
        }
    } //onClick();

    private void initializeClass() {
        // SharedPreferencesì—ì„œ ì €ì¥ëœ ì‹œê°„ ê°’ì„ ë¶ˆëŸ¬ì˜´ (ê¸°ë³¸ê°’ 120ì´ˆ)
        prefs = getSharedPreferences("AppSettings", MODE_PRIVATE);
        remainingTime = prefs.getInt("spa_test_answer_second", 120); // ì„¤ì •ëœ ê°’ ì‚¬ìš©

        // Intentì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’ -1 ì„¤ì •: ê°’ì´ ì—†ì„ ê²½ìš° ëŒ€ë¹„)
        testNum = getIntent().getIntExtra("test_num", -1);

        questionNumber = findViewById(R.id.daily_spa_examination_question_number);
        progressBar = findViewById(R.id.progress_bar);
        progressBar.setProgress(0);
        playPause = findViewById(R.id.btn_play_pause);
        playNext = findViewById(R.id.btn_play_next);
        viewImage = findViewById(R.id.image_view);
        userAnswer = findViewById(R.id.daily_spa_examination_hall_user_answer);

        playPause.setOnClickListener(this);
        playNext.setOnClickListener(this);
        viewImage.setOnClickListener(this);
        userAnswer.setOnClickListener(this);

        updateQuestionTextImage();
        viewImage.setVisibility(View.GONE);

        deleteOldMergedFiles(testNum);

        // âœ… GoogleCredentialsì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ ì €ì¥
        try {
            InputStream credentialsStream = getResources().openRawResource(R.raw.spastudyproject_key);
            googleCredentials = GoogleCredentials.fromStream(credentialsStream);
            Log.d(TAG, "GoogleCredentials loaded successfully.");
        } catch (Exception e) {
            googleCredentials = null; // ğŸ”¹ ì˜ˆì™¸ ë°œìƒ ì‹œ nullë¡œ ì„¤ì •
            Log.e(TAG, "Error loading GoogleCredentials", e);
        }

        // âœ… ë§ˆì´í¬ ê¶Œí•œ í™•ì¸
        if (ContextCompat.checkSelfPermission(this, Manifest.permission.RECORD_AUDIO)
                != PackageManager.PERMISSION_GRANTED) {
            // ê¶Œí•œ ìš”ì²­ë§Œ í•˜ê³ , ìŠ¹ì¸ í›„ ì‹¤í–‰
            ActivityCompat.requestPermissions(this,
                    new String[]{Manifest.permission.RECORD_AUDIO},
                    REQUEST_RECORD_AUDIO_PERMISSION);
        }

    }

    private void deleteOldMergedFiles(int testNum) {
        File internalDir = getFilesDir();
        File[] files = internalDir.listFiles();  // ë‚´ë¶€ ì €ì¥ì†Œì˜ ëª¨ë“  íŒŒì¼ ê°€ì ¸ì˜¤ê¸°

        if (files == null) {
            Log.e(TAG, "ë‚´ë¶€ ì €ì¥ì†Œë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.");
            return;
        }

        for (File file : files) {
            if (file.getName().matches("merged_" + testNum + "_\\d+.mp3")) {
                boolean deleted = file.delete();
                Log.d(TAG, "ì‚­ì œëœ íŒŒì¼: " + file.getAbsolutePath() + " - ì„±ê³µ ì—¬ë¶€: " + deleted);
            }
        }
    }

    @Override
    public void onRequestPermissionsResult(int requestCode, String[] permissions, int[] grantResults) {
        super.onRequestPermissionsResult(requestCode, permissions, grantResults);
        if (requestCode == REQUEST_RECORD_AUDIO_PERMISSION) {
            if (grantResults.length > 0 && grantResults[0] == PackageManager.PERMISSION_GRANTED) {
                Log.d(TAG, "Microphone permission granted. Starting recording & speech recognition...");
            } else {
                Log.e(TAG, "Microphone permission denied.");
                Toast.makeText(this, "ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì •ì—ì„œ ê¶Œí•œì„ í™œì„±í™”í•˜ì„¸ìš”.", Toast.LENGTH_LONG).show();
            }
        }
    }

    private void startPlayback() {
        if (testNum == -1 || currentIndex > 4) return;

        updateUILayout(); // ğŸ”¹ UI ë³€ê²½ ë°˜ì˜

        if(currentIndex ==  4) {
            viewImage.setVisibility(View.VISIBLE);
            // ğŸ”¹ ë™ì ìœ¼ë¡œ ì´ë¯¸ì§€ ë¦¬ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            String resourceName = "spa_test_" + testNum;
            int resId = getResources().getIdentifier(resourceName, "drawable", getPackageName());

            if (resId != 0) {
                viewImage.setImageResource(resId);
            } else {
                Log.e(TAG, "Invalid resource: " + resourceName);
            }
        }

        String fileName = "spa_test_" + testNum + "_" + currentIndex;
        int resId = getResources().getIdentifier(fileName, "raw", getPackageName());

        if (resId == 0) return;

        if (mediaPlayer != null) {
            mediaPlayer.release();
        }
        mediaPlayer = MediaPlayer.create(this, resId);
        mediaPlayer.setOnCompletionListener(mp -> {
            isMediaPlaying = false;
            isMediaPlayNow = false;
            startCountdown();
        });
        mediaPlayer.start();
        isMediaPlaying = true;
        isMediaPlayNow = true;
    }

    private void startCountdown() {
        if (timer != null) {
            timer.cancel();
        }

        progressBar.setProgress(0); // ë‹¤ìŒ ë¬¸ì œë¡œ ë„˜ì–´ê°ˆ ë•Œ í”„ë¡œê·¸ë˜ìŠ¤ë°” ì´ˆê¸°í™”
        startSpeechRecognition(); // ìŒì„± ì¸ì‹ ì‹œì‘
        startRecording();

        timer = new CountDownTimer(remainingTime * 1000L, 1000) {
            @Override
            public void onTick(long millisUntilFinished) {
                remainingTime = (int) (millisUntilFinished / 1000);
                // âœ… í”„ë¡œê·¸ë˜ìŠ¤ë°”ê°€ ì„¤ì •ëœ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì±„ì›Œì§€ë„ë¡ ì¡°ì •
                int progress = 100 - (int) ((remainingTime / (double) prefs.getInt("spa_test_answer_second", 120)) * 100);
                progressBar.setProgress(progress);
                //progressBar.setProgress(100 - (int) ((remainingTime / 120.0) * 100)); // ì™¼ìª½ë¶€í„° ê°ì†Œ
                if (remainingTime <= 10) {
                    playPause.setEnabled(false);
                    playNext.setEnabled(false);
                }
            }

            @Override
            public void onFinish() {
                Log.d(TAG, "Countdown finished. Stopping speech recognition.");

                int nowIndex = currentIndex;
                stopSpeechRecognition(); // âœ… ìŒì„± ì¸ì‹ ì¢…ë£Œ
                stopRecording();

                // ğŸ”¹ ìŒì„± ì¸ì‹ì´ ì™„ì „íˆ ì¢…ë£Œë  ë•Œê¹Œì§€ 500ms ëŒ€ê¸° í›„ `saveUserAnswer()` ì‹¤í–‰
                // âœ… 500ms í›„ `saveUserAnswer()` ì‹¤í–‰í•˜ì—¬ ì¶”ê°€ëœ ë‹¨ì–´ê°€ ë‹¤ìŒ ë¬¸ì œë¡œ ë„˜ì–´ê°€ëŠ” ê²ƒì„ ë°©ì§€
                new android.os.Handler().postDelayed(() -> {
                    saveUserAnswer(nowIndex); // âœ… ì €ì¥
                    runOnUiThread(() -> {
                        userAnswer.setText("");
                        lastRecognizedText = "";  // âœ… ë‹¤ìŒ ë¬¸ì œë¡œ ë„˜ì–´ê°ˆ ë•Œ ì´ì „ ì¸ì‹ëœ í…ìŠ¤íŠ¸ë„ ì´ˆê¸°í™”
                    });
                }, 700); // ğŸ”¹ 0.7ì´ˆ ì§€ì—° ì‹¤í–‰


                if (currentIndex == 4) {
                    saveUserAnswer(currentIndex); // âœ… ì €ì¥
                    Intent intent = new Intent(SpaExaminationHall.this, SpaExaminationResult.class);
                    intent.putExtra("test_num", testNum);
                    intent.putExtra("answer_1", answer_1);
                    intent.putExtra("answer_2", answer_2);
                    intent.putExtra("answer_3", answer_3);
                    intent.putExtra("answer_4", answer_4);
                    // âœ… ë¬¸ì œë³„ ë…¹ìŒ íŒŒì¼ ê²½ë¡œë¥¼ Intentì— ì¶”ê°€
                    if (recordedAudioFilePath_1 != null) intent.putExtra("audio_file_1", recordedAudioFilePath_1);
                    if (recordedAudioFilePath_2 != null) intent.putExtra("audio_file_2", recordedAudioFilePath_2);
                    if (recordedAudioFilePath_3 != null) intent.putExtra("audio_file_3", recordedAudioFilePath_3);
                    if (recordedAudioFilePath_4 != null) intent.putExtra("audio_file_4", recordedAudioFilePath_4);

                    startActivity(intent);
                    finish();
                    //new android.os.Handler().postDelayed(() -> finish(), 500);
                } else {
                    currentIndex++;
                    remainingTime = prefs.getInt("spa_test_answer_second", 120);
                    playPause.setEnabled(true);
                    playNext.setEnabled(true);
                    updateQuestionTextImage();
                    progressBar.setProgress(0); // ë‹¤ìŒ ë¬¸ì œë¡œ ë„˜ì–´ê°ˆ ë•Œ í”„ë¡œê·¸ë˜ìŠ¤ë°” ì´ˆê¸°í™”
                    startPlayback();
                }
            }
        };
        timer.start();
    }

    private void startSpeechRecognition() {
        if (googleCredentials == null) {
            Log.e(TAG, "GoogleCredentials is null! Speech recognition cannot start.");
            Toast.makeText(this, "ìŒì„± ì¸ì‹ API ì¸ì¦ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.", Toast.LENGTH_LONG).show();
            return;
        }

        try {
            Log.d(TAG, "Starting speech recognition...");

            // âœ… GoogleCredentials ì¬ì‚¬ìš©
            SpeechSettings speechSettings = SpeechSettings.newBuilder()
                    .setCredentialsProvider(() -> googleCredentials)
                    .build();
            speechClient = SpeechClient.create(speechSettings);

            // âœ… responseObserverë¥¼ ì˜¬ë°”ë¥´ê²Œ êµ¬í˜„ (ApiStreamObserver ì‚¬ìš©)
            ApiStreamObserver<StreamingRecognizeResponse> responseObserver = new ApiStreamObserver<StreamingRecognizeResponse>() {
                @Override
                public void onNext(StreamingRecognizeResponse response) {
                    for (StreamingRecognitionResult result : response.getResultsList()) {
                        String transcript = result.getAlternatives(0).getTranscript();
                        Log.d(TAG, "Recognized speech: " + transcript);

                        runOnUiThread(() -> {
                            if (!result.getIsFinal()) {
                                // ğŸ”¹ ì¤‘ê°„ ê²°ê³¼ë„ ëˆ„ì í•´ì„œ í‘œì‹œ
                                userAnswer.setText(lastRecognizedText + " " + transcript);
                            } else {
                                // ğŸ”¹ ìµœì¢… ê²°ê³¼ í™•ì • ì‹œ ëˆ„ì  ê°±ì‹ 
                                lastRecognizedText += " " + transcript;
                                lastRecognizedText = lastRecognizedText.trim();
                                userAnswer.setText(lastRecognizedText);
                            }
                        });


                    }
                }

                @Override
                public void onError(Throwable t) {
                    Log.e(TAG, "Speech recognition error: " + t.getMessage(), t);
                }

                @Override
                public void onCompleted() {
                    Log.d(TAG, "Speech recognition completed.");
                }
            };

            // âœ… `bidiStreamingCall()`ì„ ì˜¬ë°”ë¥´ê²Œ í˜¸ì¶œ
            BidiStreamingCallable<StreamingRecognizeRequest, StreamingRecognizeResponse> callable =
                    speechClient.streamingRecognizeCallable();

            // âœ… requestObserverì˜ íƒ€ì…ì„ ì˜¬ë°”ë¥´ê²Œ ì„¤ì • (ê°•ì œ ìºìŠ¤íŒ… ì œê±°)
            requestObserver = callable.bidiStreamingCall(responseObserver);

            // âœ… ìŒì„± ì¸ì‹ ì„¤ì • êµ¬ì„±
            RecognitionConfig config = RecognitionConfig.newBuilder()
                    .setEncoding(RecognitionConfig.AudioEncoding.LINEAR16)
                    .setSampleRateHertz(16000)
                    .setLanguageCode("en-US")
                    .build();

            StreamingRecognitionConfig streamingConfig = StreamingRecognitionConfig.newBuilder()
                    .setConfig(config)
                    .setInterimResults(true)
                    .build();

            StreamingRecognizeRequest request = StreamingRecognizeRequest.newBuilder()
                    .setStreamingConfig(streamingConfig)
                    .build();

            // âœ… ìš”ì²­ì„ ì „ì†¡
            requestObserver.onNext(request);
            Log.d(TAG, "Streaming recognition request sent.");

            isListening = true;

            // âœ… ìŒì„± ì…ë ¥ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤ë ˆë“œ ì‹¤í–‰
            executorService = Executors.newSingleThreadScheduledExecutor();
            executorService.scheduleAtFixedRate(this::streamAudio, 0, 100, TimeUnit.MILLISECONDS);

        } catch (Exception e) {
            Log.e(TAG, "Error starting speech recognition", e);
        }
    }

    private void streamAudio() {
        if (!isListening) return; // âœ… ìŒì„± ì¸ì‹ì´ ì¤‘ë‹¨ëœ ê²½ìš° ì‹¤í–‰í•˜ì§€ ì•ŠìŒ

        if (ContextCompat.checkSelfPermission(this, Manifest.permission.RECORD_AUDIO) != PackageManager.PERMISSION_GRANTED) {
            Log.e(TAG, "Microphone permission is not granted!");
            return;
        }

        if (requestObserver == null) {
            Log.e(TAG, "Speech recognition request observer is null!");
            return;
        }

        try {
            AudioRecord audioRecord = new AudioRecord(
                    MediaRecorder.AudioSource.MIC,
                    16000,
                    AudioFormat.CHANNEL_IN_MONO,
                    AudioFormat.ENCODING_PCM_16BIT,
                    AudioRecord.getMinBufferSize(16000, AudioFormat.CHANNEL_IN_MONO, AudioFormat.ENCODING_PCM_16BIT)
            );

            audioRecord.startRecording();
            Log.d(TAG, "Audio recording started.");

            byte[] buffer = new byte[4096];

            while (isListening) {
                int bytesRead = audioRecord.read(buffer, 0, buffer.length);
                if (bytesRead > 0 && requestObserver != null) {
                    try {
                        StreamingRecognizeRequest request = StreamingRecognizeRequest.newBuilder()
                                .setAudioContent(ByteString.copyFrom(buffer, 0, bytesRead))
                                .build();
                        requestObserver.onNext(request);
                    } catch (Exception e) {
                        Log.e(TAG, "Error sending audio data", e);
                        break; // âœ… ì˜ˆì™¸ ë°œìƒ ì‹œ ë£¨í”„ ì¢…ë£Œ
                    }
                }
            }

            audioRecord.stop();
            audioRecord.release();
            Log.d(TAG, "Audio recording stopped.");

        } catch (Exception e) {
            Log.e(TAG, "Error in streamAudio", e);
        }
    }

    private void stopSpeechRecognition() {
        Log.d(TAG, "Stopping speech recognition...");

        isListening = false; // âœ… ìŒì„± ì¸ì‹ ì¤‘ì§€ ìƒíƒœë¡œ ë³€ê²½

        if (requestObserver != null) {
            try {
                requestObserver.onCompleted();
                Log.d(TAG, "Request observer completed.");
            } catch (Exception e) {
                Log.e(TAG, "Error completing request observer", e);
            }
            requestObserver = null; // âœ… ìš”ì²­ ì˜µì €ë²„ë¥¼ nullë¡œ ì„¤ì •í•˜ì—¬ ì´í›„ ë°ì´í„° ì „ì†¡ ë°©ì§€
        }
        if (speechClient != null) {
            try {
                speechClient.close();
                speechClient = null;
                Log.d(TAG, "Speech client closed.");
            } catch (Exception e) {
                Log.e(TAG, "Error closing speech client", e);
            }
        }
        if (executorService != null) {
            executorService.shutdown();
            executorService = null;
            Log.d(TAG, "Executor service shut down.");
        }
    }

    private void togglePause() {
        if (isMediaPlayNow) {
            if (isMediaPlaying) {
                mediaPlayer.pause();
                isMediaPlaying = false;
            } else {
                mediaPlayer.start();
                isMediaPlaying = true;
            }
        } else if (timer != null) {
            if (isPaused) {
                startCountdown();
            } else {
                timer.cancel();
            }
            isPaused = !isPaused;
        }
    }

    private void skipToNext() {
        int nowIndex = currentIndex;
        // ìŒì„± ì¸ì‹ ì¢…ë£Œ ë° í…ìŠ¤íŠ¸ ì €ì¥
        stopSpeechRecognition(); // âœ… ìŒì„± ì¸ì‹ ì¢…ë£Œ
        stopRecording();

        // âœ… 500ms í›„ `saveUserAnswer()` ì‹¤í–‰í•˜ì—¬ ì¶”ê°€ëœ ë‹¨ì–´ê°€ ë‹¤ìŒ ë¬¸ì œë¡œ ë„˜ì–´ê°€ëŠ” ê²ƒì„ ë°©ì§€
        new android.os.Handler().postDelayed(() -> {
            saveUserAnswer(nowIndex); // âœ… ì €ì¥
            runOnUiThread(() -> {
                userAnswer.setText("");
                lastRecognizedText = "";  // âœ… ë‹¤ìŒ ë¬¸ì œë¡œ ë„˜ì–´ê°ˆ ë•Œ ì´ì „ ì¸ì‹ëœ í…ìŠ¤íŠ¸ë„ ì´ˆê¸°í™”
            });
        }, 700); // ğŸ”¹ 0.7ì´ˆ ì§€ì—° ì‹¤í–‰

        if (currentIndex == 4) {
            saveUserAnswer(currentIndex); // âœ… ì €ì¥
            Intent intent = new Intent(SpaExaminationHall.this, SpaExaminationResult.class);
            intent.putExtra("test_num", testNum);
            intent.putExtra("answer_1", answer_1);
            intent.putExtra("answer_2", answer_2);
            intent.putExtra("answer_3", answer_3);
            intent.putExtra("answer_4", answer_4);
            // âœ… ë¬¸ì œë³„ ë…¹ìŒ íŒŒì¼ ê²½ë¡œë¥¼ Intentì— ì¶”ê°€
            if (recordedAudioFilePath_1 != null) intent.putExtra("audio_file_1", recordedAudioFilePath_1);
            if (recordedAudioFilePath_2 != null) intent.putExtra("audio_file_2", recordedAudioFilePath_2);
            if (recordedAudioFilePath_3 != null) intent.putExtra("audio_file_3", recordedAudioFilePath_3);
            if (recordedAudioFilePath_4 != null) intent.putExtra("audio_file_4", recordedAudioFilePath_4);

            startActivity(intent);
            finish();
            //new android.os.Handler().postDelayed(() -> finish(), 500);
            return;
        }

        // ë¯¸ë””ì–´ê°€ ì¬ìƒ ì¤‘ì´ë©´ ì •ì§€
        if (mediaPlayer != null && isMediaPlaying) {
            mediaPlayer.stop();
            mediaPlayer.release();
            mediaPlayer = null;
            isMediaPlaying = false;
        }

        // íƒ€ì´ë¨¸ê°€ ì¡´ì¬í•˜ë©´ ì •ì§€
        if (timer != null) {
            timer.cancel();
        }

        // ë‚¨ì€ ì‹œê°„ì„ ì´ˆê¸°í™”
        remainingTime = prefs.getInt("spa_test_answer_second", 120); // ì„¤ì •ëœ ê°’ ì‚¬ìš©

        // ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì´ë™
        currentIndex++;
        updateQuestionTextImage();

        // í”„ë¡œê·¸ë˜ìŠ¤ë°” ì´ˆê¸°í™”
        progressBar.setProgress(0);

        // ë‹¤ìŒ ì§ˆë¬¸ì˜ ë¯¸ë””ì–´ë¥¼ ì¬ìƒ
        startPlayback();
    }

    // ì‚¬ìš©ìì˜ ìŒì„± ì¸ì‹ ê²°ê³¼ ì €ì¥
    private void saveUserAnswer(int nowIndex) {
        String answer = lastRecognizedText.trim(); // ğŸ”¹ ìµœì¢… ì¸ì‹ëœ ë¬¸ì¥ì„ ì €ì¥
        Log.d(TAG, "Saving user answer: " + answer);

        if (!answer.isEmpty()) {
            switch (nowIndex) {
                case 1:
                    answer_1 = answer;
                    break;
                case 2:
                    answer_2 = answer;
                    break;
                case 3:
                    answer_3 = answer;
                    break;
                case 4:
                    answer_4 = answer;
                    break;
                default:
                    Log.e(TAG, "Invalid currentIndex: " + currentIndex);
                    return;
            }

            // âœ… ì €ì¥ëœ ë‚´ìš©ì„ ë¡œê·¸ë¡œ ì¶œë ¥
            Log.d(TAG, "Answer saved -> answer_1: " + answer_1
                    + ", answer_2: " + answer_2
                    + ", answer_3: " + answer_3
                    + ", answer_4: " + answer_4);
        }

        lastRecognizedText = "";
    }

    private void updateQuestionTextImage() {
        int questionResId = getResources().getIdentifier("spa_test_question_" + currentIndex, "string", getPackageName());
        if (questionResId != 0) {
            questionNumber.setText(questionResId);
        }
        if(currentIndex == 4) {

        }
    }

    private void updateUILayout() {
        ConstraintLayout.LayoutParams userAnswerParams =
                (ConstraintLayout.LayoutParams) userAnswer.getLayoutParams();
        ConstraintLayout.LayoutParams imageParams =
                (ConstraintLayout.LayoutParams) viewImage.getLayoutParams();

        if (currentIndex == 4) {
            // ğŸ”¹ daily_spa_examination_hall_user_answer ë†’ì´ 50dp ì„¤ì •
            userAnswerParams.height = (int) TypedValue.applyDimension(
                    TypedValue.COMPLEX_UNIT_DIP, 50, getResources().getDisplayMetrics());

            // ğŸ”¹ ê¸°ì¡´ bottom constraint í•´ì œ
            userAnswerParams.bottomToBottom = ConstraintLayout.LayoutParams.UNSET;

            // ğŸ”¹ viewImageë¥¼ daily_spa_examination_hall_user_answer ì•„ë˜ë¡œ ë°°ì¹˜
            imageParams.topToBottom = userAnswer.getId();
        } else {
            // ğŸ”¹ ê¸°ì¡´ ìƒíƒœë¡œ ë³µêµ¬
            userAnswerParams.height = ConstraintLayout.LayoutParams.MATCH_CONSTRAINT;
            userAnswerParams.bottomToBottom = ConstraintLayout.LayoutParams.PARENT_ID;
            imageParams.topToBottom = ConstraintLayout.LayoutParams.UNSET;
        }

        // ğŸ”¹ ë³€ê²½ ì‚¬í•­ ì ìš©
        userAnswer.setLayoutParams(userAnswerParams);
        viewImage.setLayoutParams(imageParams);
    }

    private void startRecording() {
        if (isRecording) {
            Log.w(TAG, "Recording is already in progress. Skipping duplicate call.");
            return;
        }

        // âœ… ë§ˆì´í¬ ê¶Œí•œ í™•ì¸
        if (ContextCompat.checkSelfPermission(this, Manifest.permission.RECORD_AUDIO)
                != PackageManager.PERMISSION_GRANTED) {
            Log.e(TAG, "Microphone permission not granted. Cannot start recording.");
            return;
        }

        int bufferSize = AudioRecord.getMinBufferSize(
                16000, AudioFormat.CHANNEL_IN_MONO, AudioFormat.ENCODING_PCM_16BIT
        );

        audioRecord = new AudioRecord(
                MediaRecorder.AudioSource.MIC,
                16000,
                AudioFormat.CHANNEL_IN_MONO,
                AudioFormat.ENCODING_PCM_16BIT,
                bufferSize
        );

        byteArrayOutputStream = new ByteArrayOutputStream();
        byte[] buffer = new byte[bufferSize];

        isRecording = true;
        audioRecord.startRecording();

        // ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ë…¹ìŒ ë°ì´í„°ë¥¼ ì €ì¥
        new Thread(() -> {
            while (isRecording) {
                int bytesRead = audioRecord.read(buffer, 0, buffer.length);
                if (bytesRead > 0) {
                    byteArrayOutputStream.write(buffer, 0, bytesRead);
                }
            }
        }).start();

        Log.d(TAG, "Recording started for question " + currentIndex);
    }

    private void stopRecording() {
        if (audioRecord == null || !isRecording) {
            Log.w(TAG, "stopRecording() called but recording is not active.");
            return;
        }

        isRecording = false;
        audioRecord.stop();
        audioRecord.release();
        audioRecord = null;

        if (byteArrayOutputStream != null) {
            byte[] audioData = byteArrayOutputStream.toByteArray();
            byteArrayOutputStream = null; // ğŸ”¹ ë©”ëª¨ë¦¬ í•´ì œ

            // âœ… ìŒì„± ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ê²½ë¡œë¥¼ ì €ì¥
            String audioFilePath = saveAudioToFile(audioData, currentIndex);
            if (audioFilePath != null) {
                switch (currentIndex) {
                    case 1:
                        recordedAudioFilePath_1 = audioFilePath;
                        break;
                    case 2:
                        recordedAudioFilePath_2 = audioFilePath;
                        break;
                    case 3:
                        recordedAudioFilePath_3 = audioFilePath;
                        break;
                    case 4:
                        recordedAudioFilePath_4 = audioFilePath;
                        break;
                }
            }

            Log.d(TAG, "Recording stopped for question " + currentIndex + ", Saved at: " + audioFilePath);
        }
    }

    // ë…¹ìŒ ë°ì´í„° ì €ì¥ ë©”ì„œë“œ
    private String saveAudioToFile(byte[] audioData, int questionIndex) {
        File audioFile = new File(getFilesDir(), "audio_question_" + questionIndex + ".pcm");
        try (FileOutputStream fos = new FileOutputStream(audioFile)) {
            fos.write(audioData);
            Log.d(TAG, "Audio file saved: " + audioFile.getAbsolutePath());
            return audioFile.getAbsolutePath(); // íŒŒì¼ ê²½ë¡œ ë°˜í™˜
        } catch (IOException e) {
            Log.e(TAG, "Failed to save audio file", e);
            return null;
        }
    }

    @Override
    protected void onPause() {
        super.onPause();

        // ë¯¸ë””ì–´ê°€ ì¬ìƒ ì¤‘ì´ë©´ ì •ì§€ ë° í•´ì œ
        if (mediaPlayer != null) {
            if (isMediaPlaying) {
                mediaPlayer.stop();
                isMediaPlaying = false;
            }
            mediaPlayer.release();
            mediaPlayer = null;
        }

        // íƒ€ì´ë¨¸ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ ì •ì§€
        if (timer != null) {
            timer.cancel();
            timer = null;
        }

        stopSpeechRecognition();
        Log.d(TAG, "onPause: Stopping all media and speech recognition.");

        // ì•¡í‹°ë¹„í‹° ì¢…ë£Œ
        finish();
        //new android.os.Handler().postDelayed(() -> finish(), 500);
    }

}
